{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "salt_model_data_loader_V3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IUnFr6MO3Kk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Changes:\n",
        "1. Use Unet with Squeeze and Excitation blocks"
      ]
    },
    {
      "metadata": {
        "id": "xaO5fG0VW5gB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install required packages if running on google colab"
      ]
    },
    {
      "metadata": {
        "id": "8n6EgF7sW5gC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "c673ee57-1eaa-4e41-fbff-e9cbd261dae8"
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    !pip install torch torchvision\n",
        "    !pip install imageio\n",
        "    !pip install Augmentor\n",
        "    !git clone https://github.com/allen-q/salt_oil.git\n",
        "    !git clone https://github.com/allen-q/salt_net.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 28kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58faa000 @  0x7f834a4781c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.8MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.2.0 torch-0.4.1 torchvision-0.2.1\n",
            "Collecting imageio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (5.2.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "Successfully installed imageio-2.4.1\n",
            "Collecting Augmentor\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/f4/b0eaa9d3b4120a5450ac92d4417907ca60fad5749c1f50ed95f720792350/Augmentor-0.2.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (1.14.5)\n",
            "Requirement already satisfied: Pillow>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (5.2.0)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (0.16.0)\n",
            "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (4.26.0)\n",
            "Installing collected packages: Augmentor\n",
            "Successfully installed Augmentor-0.2.3\n",
            "Cloning into 'salt_oil'...\n",
            "remote: Counting objects: 782, done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 782 (delta 19), reused 22 (delta 9), pack-reused 738\u001b[K\n",
            "Receiving objects: 100% (782/782), 547.34 MiB | 13.96 MiB/s, done.\n",
            "Resolving deltas: 100% (466/466), done.\n",
            "Checking out files: 100% (108/108), done.\n",
            "Cloning into 'salt_net'...\n",
            "remote: Counting objects: 9349, done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 9349 (delta 63), reused 55 (delta 30), pack-reused 9261\u001b[K\n",
            "Receiving objects: 100% (9349/9349), 3.77 GiB | 16.34 MiB/s, done.\n",
            "Resolving deltas: 100% (3881/3881), done.\n",
            "Checking out files: 100% (4129/4129), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p3h4PngQ0s86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a088724-0533-4ef8-ac81-249aa65b103d"
      },
      "cell_type": "code",
      "source": [
        "cd salt_oil"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/salt_oil\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UVnBJygnW5gK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import required libs"
      ]
    },
    {
      "metadata": {
        "id": "x1VSamfH3Kk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from skimage import io, transform\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as ply\n",
        "import os\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import datetime as dt\n",
        "import pytz\n",
        "import pickle\n",
        "from salt_func_lib import *\n",
        "from torchvision import transforms, utils\n",
        "from skimage import io, transform\n",
        "import datetime as dt\n",
        "import sys\n",
        "from optparse import OptionParser\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from io import BytesIO\n",
        "import Augmentor\n",
        "from Augmentor.Operations import *\n",
        "import random\n",
        "import PIL\n",
        "import cv2 as cv\n",
        "% matplotlib inline\n",
        "% load_ext autoreload\n",
        "% autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I87qLhAOW5gO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Unet Modules"
      ]
    },
    {
      "metadata": {
        "id": "eC32auGDqVZi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pytorch_unet.eval import eval_net\n",
        "from pytorch_unet.unet import UNet\n",
        "from pytorch_unet.unet.unet_parts import *\n",
        "from pytorch_unet.unet.resnet import *\n",
        "from pytorch_unet.utils import get_ids, split_ids, split_train_val, get_imgs_and_masks, batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6O8Wz9H_iTDE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Setup data type based on whether GPU is enabled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QKYhIfCtEk6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    dtype = torch.cuda.FloatTensor ## UNCOMMENT THIS LINE IF YOU'RE ON A GPU!\n",
        "else:    \n",
        "    dtype = torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51wk3a_bTtv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c8c825f-f5e1-48dd-c628-3d0d56190a15"
      },
      "cell_type": "code",
      "source": [
        "print(f'Data Type set to: {dtype}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Type set to: <class 'torch.cuda.FloatTensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "30EV6nbKbtyV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Global Variables"
      ]
    },
    {
      "metadata": {
        "id": "8zcHyi6AMfDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_global_variables():\n",
        "    \"\"\"initialize global variables such as db connection, logger etc.\"\"\"\n",
        "    global log\n",
        "    log = get_logger('SaltNet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLILkc8Vbtya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init_global_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DNoyvMPbFOF6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def adjust_brightness(img, alpha=None, beta=None):\n",
        "    if alpha is None:\n",
        "        # get a random num from 0.75 to 1.25\n",
        "        alpha = (random.random()/2)+0.75\n",
        "    if beta is None:\n",
        "        # get a random num from -40 to 40\n",
        "        beta = round((random.random()-0.5)*80)\n",
        "    #print(f'a:{alpha}, b:{beta}')\n",
        "    img_new = cv.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "    return img_new.reshape(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwNexNnZxQ7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SaltDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, np_img, np_mask, df_depth, mean_img, out_size=101, \n",
        "                 out_ch=1, transform=None, random_brightness=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (string): Path to the image files.\n",
        "            train (bool): Load train or test data\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.np_img = np_img\n",
        "        self.np_mask = np_mask.clip(0,1)\n",
        "        self.df_depth = df_depth\n",
        "        self.mean_img = mean_img\n",
        "        self.out_size = out_size\n",
        "        self.out_ch = out_ch\n",
        "        self.transform = transform\n",
        "        self.random_brightness = random_brightness\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.np_img)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, torch.Tensor):\n",
        "            idx = idx.item()\n",
        "            \n",
        "        X = self.np_img[idx]\n",
        "        #X = X - self.mean_img\n",
        "\n",
        "        if self.np_mask is None:\n",
        "            y = np.zeros((101,101,1))\n",
        "        else:\n",
        "            y = self.np_mask[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img_in = PIL.Image.fromarray(np.c_[np.tile(X, 2), y*255])\n",
        "            #img_in = PIL.Image.fromarray(np.tile(y, 3)*255)\n",
        "            transformed = np.array(self.transform(img_in))\n",
        "            #X = np.clip(transformed[:,:,0:1]/255, 0., 1.) - self.mean_img\n",
        "            X = transformed[:,:,0:1]\n",
        "            y = np.clip(transformed[:,:,2:3]/255, 0., 1.)\n",
        "            \n",
        "        if self.random_brightness > random.random():            \n",
        "            X = adjust_brightness(X)\n",
        "            X = np.clip(X/255, 0., 1.) - self.mean_img\n",
        "        else:\n",
        "            X = np.clip(X/255, 0., 1.) - self.mean_img            \n",
        "        #from boxx import g\n",
        "        #g()\n",
        "        X = np.moveaxis(X, -1,0)\n",
        "\n",
        "        pad_size = self.out_size - X.shape[2]\n",
        "        pad_first = pad_size//2\n",
        "        pad_last = pad_size - pad_first\n",
        "        X = np.pad(X, [(0, 0),(pad_first, pad_last), (pad_first, pad_last)], mode='reflect')\n",
        "\n",
        "        d = self.df_depth.iloc[idx,0]\n",
        "\n",
        "        X = torch.from_numpy(X).float().type(dtype)\n",
        "        X = X.repeat(self.out_ch,1,1)\n",
        "        y = transform.resize(y, (101, 101), mode='constant', preserve_range=True)\n",
        "        y = torch.from_numpy(y).ge(0.5).float().squeeze().type(dtype)\n",
        "\n",
        "        return (X,y,d,idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q0_gBiSvxQ7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Pipeline_Salt(Augmentor.Pipeline):\n",
        "    def __init__(self, source_directory=None, output_directory=\"output\", min_mask_ratio=0., save_format=None):\n",
        "        super(Pipeline_Salt, self).__init__(source_directory, output_directory, save_format)\n",
        "        self.min_mask_ratio = min_mask_ratio\n",
        "\n",
        "    def torch_transform(self):\n",
        "        \"\"\"\n",
        "        Returns the pipeline as a function that can be used with torchvision.\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            >>> import Augmentor\n",
        "            >>> import torchvision\n",
        "            >>> p = Augmentor.Pipeline()\n",
        "            >>> p.rotate(probability=0.7, max_left_rotate=10, max_right_rotate=10)\n",
        "            >>> p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "            >>> transforms = torchvision.transforms.Compose([\n",
        "            >>>     p.torch_transform(),\n",
        "            >>>     torchvision.transforms.ToTensor(),\n",
        "            >>> ])\n",
        "\n",
        "        :return: The pipeline as a function.\n",
        "        \"\"\"\n",
        "        def _transform(image):\n",
        "            for operation in self.operations:\n",
        "                r = round(random.uniform(0, 1), 1)\n",
        "                if r <= operation.probability:\n",
        "\n",
        "                    if not isinstance(image, list):\n",
        "                        image = [image]                        \n",
        "                    #print(type(operation))\n",
        "                    #print(np.array(image[0]).shape)                        \n",
        "                    if isinstance(operation, (Crop, CropPercentage)):\n",
        "                        image = _min_mask_crop(image, operation, min_mask_ratio=self.min_mask_ratio)\n",
        "                    else:\n",
        "                        image = operation.perform_operation(image)[0]\n",
        "\n",
        "            return image\n",
        "        def _min_mask_crop(image, operation, min_mask_ratio=0.):\n",
        "            '''Crop a image with the mask area not less than min_mask_ratio*original mask area.'''\n",
        "            img_np = np.array(image[0])\n",
        "            mask_in = img_np[:,:,2]\n",
        "            mask_in_pct = (mask_in>0).sum()/mask_in.size\n",
        "            for i in range(10):                    \n",
        "                img_out = operation.perform_operation(image)[0]\n",
        "                mask_out = np.array(img_out)[:,:,2]\n",
        "                mask_out_pct = (mask_out>0).sum()/mask_out.size\n",
        "                #print(mask_out_pct)\n",
        "                if mask_out_pct >= min_mask_ratio*mask_in_pct:\n",
        "                    break\n",
        "            #print(f'min_mask_ratio:{min_mask_ratio} in pct: {mask_in_pct}, out pct: {mask_out_pct}')\n",
        "            return img_out\n",
        "            \n",
        "        return _transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEXPdEFd3KmA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "metadata": {
        "id": "53yVOPsQ3KmB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load train and test data from npy files or from raw images if npy files not exist."
      ]
    },
    {
      "metadata": {
        "id": "wO1kf6HW3KmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2dd57500-8049-4e96-f8ed-092b6014544e"
      },
      "cell_type": "code",
      "source": [
        "np_train_all, np_train_all_mask, X_test, misc_data = load_all_data()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Try loading data from npy and pickle files...\n",
            "Data loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fUAORGGdEgJx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate number of mask pixels per image"
      ]
    },
    {
      "metadata": {
        "id": "Hyt5JXyrEgKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train_mask = pd.DataFrame((np_train_all_mask/255).sum((1,2,3)), columns=['mask_pix'])\n",
        "\n",
        "df_train_mask.mask_pix = df_train_mask.mask_pix.round(-2)\n",
        "\n",
        "mask_pix_bins = df_train_mask.mask_pix.sort_values().unique()\n",
        "# Due to zooming and crop, under-sample all black and all white masks, over-sample images with small mask areas.\n",
        "mask_pix_bin_weights = ([1.] + np.r_[2:1:102j].tolist() + [0.2])\n",
        "mask_pix_bin_weights = dict(zip(mask_pix_bins, mask_pix_bin_weights))\n",
        "\n",
        "train_all_sample_weight = df_train_mask.mask_pix.map(mask_pix_bin_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq_sH6YeEgKG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#np.log10(df_train_mask.mask_pix.div(100).add(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTkpzBqmGRN2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove black images"
      ]
    },
    {
      "metadata": {
        "id": "8BXkNsiZGRN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#black_img_ids = (np_train_all.max((1,2,3))==0)\n",
        "\n",
        "#np_train_all = np_train_all[~black_img_ids]\n",
        "#np_train_all_mask = np_train_all_mask[~black_img_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "obPeKNjDGRN6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22064789-5a34-4e0d-89d8-bf7e3ad4712b"
      },
      "cell_type": "code",
      "source": [
        "np_train_all.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000, 101, 101, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "DZqPs7VnYd56",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove images with all black masks"
      ]
    },
    {
      "metadata": {
        "id": "lzqSzGzEYd5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#black_mask_ids = (np_train_all_mask.max((1,2,3))==0)\n",
        "#np_train_all = np_train_all[~black_mask_ids]\n",
        "#np_train_all_mask = np_train_all_mask[~black_mask_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DNIS7zT23KmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Val data split"
      ]
    },
    {
      "metadata": {
        "id": "Q8HLh-bNQmNz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#np_train_all = np.clip(np_train_all/255, 0, 1)\n",
        "#X_test = np.clip(X_test/255, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqYjA-Ud3KmI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_ids, X_val_ids = (\n",
        "    train_test_split(df_train_mask.index.tolist(), \n",
        "                     test_size=0.20,\n",
        "                     stratify = df_train_mask.mask_pix,\n",
        "                     random_state=0)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GAWRi1bLpyM_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('./data/df_train_img_iou.pickle', 'rb') as f:\n",
        "    df_train_img_iou = pickle.load(f)\n",
        "\n",
        "train_hard_img_id= (\n",
        "    [misc_data['np_train_all_ids'].index(e) for e in df_train_img_iou.loc[df_train_img_iou.type=='HARD'].id]\n",
        ")\n",
        "\n",
        "#X_train_ids = np.setdiff1d(X_train_ids, train_hard_img_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GEqXO7GM3KmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = np_train_all[X_train_ids]\n",
        "X_val = np_train_all[X_val_ids]\n",
        "y_train = np_train_all_mask[X_train_ids]\n",
        "y_val = np_train_all_mask[X_val_ids]\n",
        "depth_train = (\n",
        "    misc_data['df_train_all_depth']\n",
        "    .reindex(np.array(misc_data['np_train_all_ids'])[X_train_ids])\n",
        ")\n",
        "depth_val = (\n",
        "    misc_data['df_train_all_depth']\n",
        "    .reindex(np.array(misc_data['np_train_all_ids'])[X_val_ids])\n",
        ")\n",
        "depth_test = (\n",
        "    misc_data['df_train_all_depth']\n",
        "    .reindex(np.array(misc_data['np_test_ids']))\n",
        ")\n",
        "#X_train_mean_img = X_train.mean(0).astype(np.float32)\n",
        "#X_train_mean_img = X_train.mean((0,1,2)).astype(np.float32)\n",
        "X_train_mean_img = np.clip(np_train_all/255, 0, 1).mean((0,1,2)).astype(np.float32)\n",
        "\n",
        "all_data = {\n",
        "    'X_train': X_train,\n",
        "    'X_val': X_val,\n",
        "    'y_train': y_train,\n",
        "    'y_val': y_val,\n",
        "    'X_test': X_test,\n",
        "    'X_train_mean_img': np.zeros_like(X_train_mean_img)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6BSSFObEgKl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_sample_weight = train_all_sample_weight[X_train_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ZVs5X-EpyNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-dw2YwtbSmHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_mean_img = np.zeros_like(X_train_mean_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5cc6IFsXVU3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb583aa7-f582-4b95-c7b2-6cca99db2eaa"
      },
      "cell_type": "code",
      "source": [
        "X_train_mean_img"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "2L51fSDkpyNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe93f056-71b5-43f8-e8b4-3c97c2b37a9c"
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3200, 101, 101, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "zl5Q4dsOpyNa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46fc9a87-4a0c-42e9-b874-8cd27e1709cd"
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3200, 101, 101, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "koLKXAYPpyNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "771fef01-4040-4df2-f02a-aa4f95b10db0"
      },
      "cell_type": "code",
      "source": [
        "depth_train.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3200, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "MfYxl3YIR9LG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "3f2d93ac-f80b-428d-cf75-d7bbf38fa73f"
      },
      "cell_type": "code",
      "source": [
        "'''p = Pipeline_Salt(min_mask_ratio=0.9)\n",
        "p.crop_random(probability=1, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
        "img = np.c_[np.tile(X_train[469], 2), y_train[469]]\n",
        "img_in = PIL.Image.fromarray(img)\n",
        "tsfm = p.torch_transform()\n",
        "img_out = tsfm(img_in)'''"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"p = Pipeline_Salt(min_mask_ratio=0.9)\\np.crop_random(probability=1, percentage_area=0.8, randomise_percentage_area=False)\\np.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\\nimg = np.c_[np.tile(X_train[469], 2), y_train[469]]\\nimg_in = PIL.Image.fromarray(img)\\ntsfm = p.torch_transform()\\nimg_out = tsfm(img_in)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "0bMOyfebQmON",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt(min_mask_ratio=0.1)\n",
        "p.skew(probability=0.5, magnitude=0.2)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
        "p.rotate(probability=0.5, max_left_rotation=5, max_right_rotation=5)\n",
        "#p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qne_6pQph0dV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8eXIcYoDqVcF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a Train Dataloader"
      ]
    },
    {
      "metadata": {
        "id": "uYerp5hjW5gu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     #'sampler': weighted_sampler,\n",
        "                     'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n",
        "\n",
        "sample = iter(dataloaders['train']).__next__()\n",
        "\n",
        "assert sample[0].shape == torch.Size([train_data_params['batch_size'], 1, 128, 128])\n",
        "assert sample[1].shape == torch.Size([train_data_params['batch_size'], 101, 101])\n",
        "assert sample[2].shape == torch.Size([train_data_params['batch_size']])\n",
        "assert sample[3].shape == torch.Size([train_data_params['batch_size']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHZ6rNjxFOIB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = iter(train_dataLoader).__next__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fgi872V02Fsz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch, d_batch, X_id = t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dp1YZ7D5QmOg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for X_batch, y_batch, d_batch, X_id in dataloaders['train']:\n",
        "    i+=1\n",
        "    if i>30:\n",
        "        break\n",
        "    X_orig = X_train[X_id[0]].squeeze()/255\n",
        "    X_tsfm = X_batch[0,0].squeeze().cpu().detach().numpy()[13:114,13:114] + X_train_mean_img.squeeze()\n",
        "    y_orig = y_train[X_id[0]].squeeze()\n",
        "    y_tsfm = (y_batch[0].squeeze().cpu().detach().numpy())\n",
        "    print(f'{X_orig.sum()}, {X_tsfm.sum()}')\n",
        "    plot_img_mask_pred([X_orig, X_tsfm, y_orig, y_tsfm],\n",
        "                       [f'X Original-{X_id[0]}', 'X Transformed', 'y Original', 'y Transformed'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ars8S4bvf80",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qS4kZWQnW5gw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a Train Dataloader for sanity check"
      ]
    },
    {
      "metadata": {
        "id": "lP-yemq8pyOf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(depth_train['weight'][:8], 2)#weight \n",
        "#weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values[:8], 2)\n",
        "train_data_params = {'batch_size': 2, 'shuffle': True,}\n",
        "val_data_params = {'batch_size': 2, 'shuffle': True,}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEA7t3xaQmOo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataLoader  = (\n",
        "    DataLoader(SaltDataset(X_train[:4], y_train[:4], depth_train[:4],\n",
        "                           X_train_mean_img, out_size=128, out_ch=1,\n",
        "                           transform=None), **train_data_params)\n",
        "                           #transform=p.torch_transform()), **data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val[:4], y_val[:4], depth_val[:4], \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2hyd9ryGROf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = iter(train_dataLoader).__next__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v34uY0GkGROl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch, d_batch, X_id = t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GjWYq0PnjVlT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb87d7f2-2262-4eac-c605-4bfa79de2734"
      },
      "cell_type": "code",
      "source": [
        "X_batch[0].max()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "b3NeHMttxPA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_iter_stats(y_pred, y_batch, X_batch, X_id, train_params, other_data, epoch_losses, epoch, iter_count, start):\n",
        "    #from boxx import g\n",
        "    #g(), \n",
        "    epoch_losses = [round(e.item(),4) for e in torch.stack(epoch_losses).mean(0)]\n",
        "    iou_batch = calc_mean_iou(y_pred.ge(train_params['mask_cutoff']), y_batch)\n",
        "    iou_acc = calc_clf_accuracy(y_pred.ge(train_params['mask_cutoff']), y_batch)\n",
        "\n",
        "    log.info('Losses: {}, Batch IOU: {:.4f}, Batch Acc: {:.4f} at iter {}, epoch {}, Time: {}'.format(\n",
        "            epoch_losses, iou_batch, iou_acc, iter_count, epoch, timeSince(start))\n",
        "    )\n",
        "\n",
        "    X_train = other_data['X_train']\n",
        "    y_train = other_data['y_train']\n",
        "    X_train_mean_img = other_data['X_train_mean_img']\n",
        "    #print(all_losses)\n",
        "    X_orig = X_train[X_id[0]].squeeze()/255\n",
        "    X_tsfm = X_batch[0,0].squeeze().cpu().detach().numpy()\n",
        "    X_tsfm = X_tsfm[13:114,13:114] + X_train_mean_img.squeeze()\n",
        "    y_orig = y_train[X_id[0]].squeeze()\n",
        "    y_tsfm = (y_batch[0].squeeze().cpu().detach().numpy())\n",
        "    y_tsfm_pred =  y_pred[0].squeeze().gt(train_params['mask_cutoff'])\n",
        "    plot_img_mask_pred([X_orig, X_tsfm, y_orig, y_tsfm, y_tsfm_pred],\n",
        "                       ['X Original', 'X Transformed', 'y Original', 'y Transformed', 'y Predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7DT72CtdxPA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_epoch_stats(model, y_pred, X_id, other_data, pred_vs_true_epoch, train_params, phase, epoch, iter_count, best_iou, all_losses, best_model):    \n",
        "    y_pred_epoch = torch.cat([e[0] for e in pred_vs_true_epoch])\n",
        "    y_true_epoch = torch.cat([e[1] for e in pred_vs_true_epoch])\n",
        "\n",
        "    mean_iou_epoch = calc_mean_iou(y_pred_epoch.ge(train_params['mask_cutoff']), y_true_epoch.float())\n",
        "    mean_acc_epoch = calc_clf_accuracy(y_pred_epoch.ge(train_params['mask_cutoff']), y_true_epoch.float())\n",
        "    log.info('{} Mean IOU: {:.4f}, Mean Acc: {:.4f}, Best Val IOU: {:.4f} at epoch {}'.format(phase, mean_iou_epoch, mean_acc_epoch, best_iou, epoch))\n",
        "\n",
        "    if phase == 'val' and mean_iou_epoch > best_iou:\n",
        "        best_iou = mean_iou_epoch\n",
        "        stats = {'best_iou': best_iou,\n",
        "                 'all_losses': all_losses,\n",
        "                 'iter_count': iter_count}\n",
        "        best_model = (epoch, copy.deepcopy(model.state_dict()),\n",
        "                                            copy.deepcopy(optimizer.state_dict()),\n",
        "                                            copy.deepcopy(scheduler.state_dict()), stats, train_params['model_save_name'], '.')\n",
        "        log.info(save_model_state_to_chunks(*best_model))\n",
        "        \n",
        "        log.info('Best Val Mean IOU so far: {}'.format(best_iou))\n",
        "        # Visualize 1 val sample and predictions\n",
        "        X_val = other_data['X_val']\n",
        "        y_val = other_data['y_val']\n",
        "        X_orig = X_val[X_id[0]].squeeze()/255\n",
        "        y_orig = y_val[X_id[0]].squeeze()\n",
        "        y_pred2 =  y_pred[0].squeeze().gt(train_params['mask_cutoff'])\n",
        "        plot_img_mask_pred([X_orig, y_orig, y_pred2],\n",
        "                           ['Val X Original', 'Val y Original', 'Val y Predicted'])\n",
        "\n",
        "    return best_iou, best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfa9Y5hlxPBA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_model_to_git(epoch, save_model_every, num_epochs, prev_best_iou, best_iou, \n",
        "                      model_save_iou_threshold, best_model, model_save_name):\n",
        "    if (epoch % save_model_every== 0) | (epoch == num_epochs-1):\n",
        "        if (best_model is not None) and (best_iou > model_save_iou_threshold):\n",
        "            log.info(save_model_state_to_chunks(*best_model))\n",
        "            push_model_to_git(ckp_name=model_save_name)\n",
        "            prev_best_iou = best_iou\n",
        "        else:\n",
        "            log.info(\"Skip pushing model to git as there's no improvement\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-pRk5cqxPBE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_loss(y_pred, y_batch, loss_fns, loss_fn_weights):\n",
        "     losses = []\n",
        "     for loss_fn, loss_fn_weight in zip(loss_fns, loss_fn_weights):\n",
        "         loss = loss_fn_weight * loss_fn(y_pred, y_batch)\n",
        "         losses.append(loss)  \n",
        "\n",
        "     return torch.stack(losses + [torch.stack(losses).sum()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pGqBDcs3xPBF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, loss_fns, loss_fn_weights, optimizer, scheduler, train_params, other_data):\n",
        "    log.info('Start Training...')    \n",
        "    log.info((model, dataloaders, loss_fns, loss_fn_weights, optimizer, scheduler, train_params))\n",
        "    num_epochs = train_params['num_epochs']\n",
        "    start = time.time()\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    best_model = None\n",
        "    best_iou = prev_best_iou = 0.0    \n",
        "    all_losses = []\n",
        "    iter_count = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        log.info('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        log.info('-' * 20)\n",
        "        if (epoch % train_params['save_log_every'] == 0):\n",
        "            push_log_to_git()\n",
        "        epoch_losses = []\n",
        "        for phase in ['train', 'val']:\n",
        "            model.train() if phase == 'train' else model.eval()      \n",
        "            pred_vs_true_epoch = []\n",
        "            for X_batch, y_batch, d_batch, X_id in dataloaders[phase]:\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    y_pred = model(X_batch)\n",
        "                    pred_vs_true_epoch.append([y_pred, y_batch])\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        losses = calc_loss(y_pred, y_batch.float(), loss_fns, loss_fn_weights)\n",
        "                        epoch_losses.append(losses)\n",
        "                        all_losses.append(losses)\n",
        "                        loss = losses[-1]\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        iter_count += 1\n",
        "                if (phase == 'train') & (iter_count % train_params['print_every'] == 0):\n",
        "                    log_iter_stats(y_pred, y_batch, X_batch, X_id, train_params, other_data, epoch_losses, epoch, iter_count, start)\n",
        "            best_iou, best_model = log_epoch_stats(model, y_pred, X_id, other_data, pred_vs_true_epoch, train_params, \n",
        "                                                   phase, epoch, iter_count, best_iou, all_losses, best_model)\n",
        "            \n",
        "        save_model_to_git(epoch, train_params['save_model_every'], num_epochs, prev_best_iou, best_iou,\n",
        "                          train_params['model_save_iou_threshold'], best_model, model_save_name)\n",
        "        #from boxx import g\n",
        "        #g()\n",
        "        epoch_avg_loss = np.mean([e[-1].item() for e in epoch_losses])\n",
        "        #print(epoch_avg_loss)\n",
        "        scheduler.step(epoch_avg_loss)\n",
        "\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model[1])\n",
        "    log.info('-' * 20)\n",
        "    log.info(f'Training complete in {(time.time() - start) // 60} mins. Best Val IOU {round(best_iou, 4)}')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dasEpZc0QmOt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model using a small data set to see if it can overfit"
      ]
    },
    {
      "metadata": {
        "id": "2OoY1-HV8DZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bff386c8-7a2d-4f8e-d79c-de089bd7658a"
      },
      "cell_type": "code",
      "source": [
        "#saltnet = resnet34unet(in_ch=3, bilinear=False, pretrained=False)\n",
        "saltnet = UResNet(pretrained=True)\n",
        "\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "#loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "\n",
        "#scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True, threshold=0.001)\n",
        "model_save_name = None\n",
        "\n",
        "# Test Run\n",
        "#trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "#                other_data=all_data, num_epochs=100, print_every=8, save_model_every=None, save_log_every=None, log=log, loss2_weight=0.0)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet using pretrained weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zn2zdk8E4oVi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True, threshold=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyLX1EVIxPBQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_params = {\n",
        "    'model_save_name': f'../salt_net/test_{get_current_time_as_fname()}.ckp',\n",
        "    'save_model_every': 10000,\n",
        "    'save_log_every': 100,\n",
        "    'num_epochs': 80,\n",
        "    'print_every': 2,\n",
        "    'log': log,\n",
        "    'mask_cutoff': 0,\n",
        "    'model_save_iou_threshold': 0.1\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pE7Q6dd3xPBR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model(saltnet, dataloaders, (loss_fn_bce, loss_lovasz_hinge), (1, 0.5), optimizer, scheduler, train_params, all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvpnKQJovwwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install boxx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zJ3DJ4hAQmOw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the full with full dataset"
      ]
    },
    {
      "metadata": {
        "id": "AZkSxKV2sIkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62e80fd2-46b2-4aaf-d6b8-c9966e480973"
      },
      "cell_type": "code",
      "source": [
        "log.info('Use Unet with Resnet 34 as backbone run 4 with SGD and updated train_model function')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/09/2018 04:48:31 - SaltNet - INFO - Use Unet with Resnet 34 as backbone run 3 with SGD.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "z3treVXh_Uha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_config = '''\n",
        "\n",
        "p = Pipeline_Salt(min_mask_ratio=0.1)\n",
        "#p.skew(probability=0.5, magnitude=0.2)\n",
        "#p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     #'sampler': weighted_sampler,\n",
        "                     'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           np.zeros_like(X_train_mean_img), out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           np.zeros_like(X_train_mean_img), out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n",
        "\n",
        "#saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
        "saltnet = UResNet(pretrained=True)\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "#loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.SGD(saltnet.parameters(), lr=0.02)\n",
        "#scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, min_lr=0.000001)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_res34_bce_lovasz_loss_se_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yP2IQIkOD584",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "3d73f702-09ca-42af-a8ef-226a2a85bc51"
      },
      "cell_type": "code",
      "source": [
        "log.info(model_config)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/09/2018 15:12:05 - SaltNet - INFO - \n",
            "\n",
            "p = Pipeline_Salt(min_mask_ratio=0.1)\n",
            "#p.skew(probability=0.5, magnitude=0.2)\n",
            "#p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
            "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
            "p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
            "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
            "p.flip_left_right(probability=0.5)\n",
            "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
            "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
            "\n",
            "train_data_params = {'batch_size': 32,\n",
            "                     #'sampler': weighted_sampler,\n",
            "                     'shuffle': True,\n",
            "                     'drop_last': True}\n",
            "\n",
            "val_data_params = {'batch_size': 32,\n",
            "                   'shuffle': True,\n",
            "                   'drop_last': False}\n",
            "\n",
            "train_dataLoader = (\n",
            "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
            "                           np.zeros_like(X_train_mean_img), out_size=128,  out_ch=1,\n",
            "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
            ")\n",
            "\n",
            "val_dataLoader = (\n",
            "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
            "                           np.zeros_like(X_train_mean_img), out_size=128, out_ch=1), **val_data_params)\n",
            ")\n",
            "\n",
            "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n",
            "\n",
            "#saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
            "saltnet = UResNet(pretrained=True)\n",
            "loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
            "#loss_focal = FocalLoss(0.25, 2, logits=True)\n",
            "loss_lovasz_hinge = LovaszHingeLoss()\n",
            "optimizer = torch.optim.SGD(saltnet.parameters(), lr=0.02)\n",
            "#scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
            "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, min_lr=0.000001)\n",
            "\n",
            "model_save_name = f'../salt_net/Unet_res34_bce_lovasz_loss_se_{get_current_time_as_fname()}.ckp'\n",
            "log.info(model_save_name)\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ikq0ObjsDXc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt(min_mask_ratio=0.1)\n",
        "#p.skew(probability=0.5, magnitude=0.2)\n",
        "#p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     #'sampler': weighted_sampler,\n",
        "                     'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           np.zeros_like(X_train_mean_img), out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           np.zeros_like(X_train_mean_img), out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u8EUxk8UQmOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f770e049-8388-4293-9738-9e8a07ee866f"
      },
      "cell_type": "code",
      "source": [
        "#saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
        "saltnet = UResNet(pretrained=True)\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "#loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.SGD(saltnet.parameters(), lr=0.02)\n",
        "#scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, min_lr=0.000001)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_res34_bce_lovasz_loss_se_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet using pretrained weights.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "17/09/2018 15:12:39 - SaltNet - INFO - ../salt_net/Unet_res34_bce_lovasz_loss_se_2018_09_18_01_12_39.ckp\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wq1P9vlMDo06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_params = {\n",
        "    'model_save_name': model_save_name,\n",
        "    'save_model_every': 20,\n",
        "    'save_log_every': 2,\n",
        "    'num_epochs': 250,\n",
        "    'print_every': 50,\n",
        "    'log': log,\n",
        "    'mask_cutoff': 0.,\n",
        "    'model_save_iou_threshold': 0.79\n",
        "    }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H2z9Brb8EgMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6792
        },
        "outputId": "874a57f1-e5a5-4402-fb28-9ee42d9885ae"
      },
      "cell_type": "code",
      "source": [
        "train_model(saltnet, dataloaders, (loss_fn_bce, loss_lovasz_hinge), (1, 0.1), optimizer, scheduler, train_params, all_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/09/2018 15:12:54 - SaltNet - INFO - Start Training...\n",
            "17/09/2018 15:12:54 - SaltNet - INFO - (UResNet(\n",
            "  (resnet): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
            "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            "  )\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "  )\n",
            "  (encoder2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (encoder3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (encoder4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (encoder5): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (center): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (decoder5): Decoder(\n",
            "    (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (se): Seq_Ex_Block(\n",
            "      (se): Sequential(\n",
            "        (0): GlobalAvgPool()\n",
            "        (1): Linear(in_features=64, out_features=4, bias=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Linear(in_features=4, out_features=64, bias=True)\n",
            "        (4): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder4): Decoder(\n",
            "    (conv1): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (se): Seq_Ex_Block(\n",
            "      (se): Sequential(\n",
            "        (0): GlobalAvgPool()\n",
            "        (1): Linear(in_features=64, out_features=4, bias=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Linear(in_features=4, out_features=64, bias=True)\n",
            "        (4): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder3): Decoder(\n",
            "    (conv1): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (se): Seq_Ex_Block(\n",
            "      (se): Sequential(\n",
            "        (0): GlobalAvgPool()\n",
            "        (1): Linear(in_features=64, out_features=4, bias=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Linear(in_features=4, out_features=64, bias=True)\n",
            "        (4): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder2): Decoder(\n",
            "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (se): Seq_Ex_Block(\n",
            "      (se): Sequential(\n",
            "        (0): GlobalAvgPool()\n",
            "        (1): Linear(in_features=64, out_features=4, bias=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Linear(in_features=4, out_features=64, bias=True)\n",
            "        (4): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder1): Decoder(\n",
            "    (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (se): Seq_Ex_Block(\n",
            "      (se): Sequential(\n",
            "        (0): GlobalAvgPool()\n",
            "        (1): Linear(in_features=64, out_features=4, bias=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Linear(in_features=4, out_features=64, bias=True)\n",
            "        (4): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (se_f): Seq_Ex_Block(\n",
            "    (se): Sequential(\n",
            "      (0): GlobalAvgPool()\n",
            "      (1): Linear(in_features=320, out_features=20, bias=True)\n",
            "      (2): ReLU(inplace)\n",
            "      (3): Linear(in_features=20, out_features=320, bias=True)\n",
            "      (4): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (outc): OutConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace)\n",
            "      (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (sig): Sigmoid()\n",
            "  )\n",
            "), {'train': <torch.utils.data.dataloader.DataLoader object at 0x7f65e4245898>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f65e4245b00>}, (BCEWithLogitsLoss(), LovaszHingeLoss()), (1, 0.1), SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.02\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f663b94e0b8>, {'model_save_name': '../salt_net/Unet_res34_bce_lovasz_loss_se_2018_09_18_01_12_39.ckp', 'save_model_every': 20, 'save_log_every': 2, 'num_epochs': 250, 'print_every': 50, 'log': <Logger SaltNet (DEBUG)>, 'mask_cutoff': 0.0, 'model_save_iou_threshold': 0.79})\n",
            "17/09/2018 15:12:54 - SaltNet - INFO - Epoch 1/250\n",
            "17/09/2018 15:12:54 - SaltNet - INFO - --------------------\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "UkzkKviGQ1ME",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fine Tune"
      ]
    },
    {
      "metadata": {
        "id": "6gH3StpeyZoc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info('Fine tune Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34 net. Use loss_lovasz_hinge loss only.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1G7CTpsySfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_config = '''\n",
        "p = Pipeline_Salt(min_mask_ratio=0.5)\n",
        "p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
        "\n",
        "saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "saltnet.load_state_dict(model_state_dict['model'])\n",
        "\n",
        "#saltnet = loaded_model\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss()\n",
        "#loss_focal = FocalLoss(0.25, 2.5, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_lovasz_loss_se_finetune2_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=60, print_every=50, save_model_every=20, save_log_every=2, log=log, loss1_weight=0.0, loss2_weight=1.0)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLGTRHvqzIMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info(model_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-WDeSLljRAW8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd ../salt_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MfnNZyIMjr2z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "saltnet.load_state_dict(model_state_dict['model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1TB1NlPx-_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#saltnet = loaded_model\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss()\n",
        "#loss_focal = FocalLoss(0.25, 2.5, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_lovasz_loss_se_finetune2_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=100, print_every=44, save_model_every=20, save_log_every=2, log=log, loss1_weight=0.0, loss2_weight=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OX55uUxAU4dF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "2879//32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZOM1E8wSiIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fine tune 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F1lR_tZsSiTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info('Fine tune Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34 net. Remove hard images. Use loss_lovasz_hinge and bce loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBQUK1P6T-5y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_config = '''\n",
        "p = Pipeline_Salt(min_mask_ratio=0.5)\n",
        "p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
        "\n",
        "saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "saltnet.load_state_dict(model_state_dict['model'])\n",
        "\n",
        "#saltnet = loaded_model\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss()\n",
        "#loss_focal = FocalLoss(0.25, 2.5, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_bce_loss_lovasz_loss_se_finetune2_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=100, print_every=50, save_model_every=20, save_log_every=2, log=log, loss1_weight=1.0, loss2_weight=1.0)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLJXcQFjUAw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info(model_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eXVLu2iuTuQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd ../salt_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mp4vCVJSvMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "saltnet.load_state_dict(model_state_dict['model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mwx6Lf5GSiaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#saltnet = loaded_model\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss()\n",
        "#loss_focal = FocalLoss(0.25, 2.5, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_bce_loss_lovasz_loss_se_finetune3_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=100, print_every=44, save_model_every=20, save_log_every=2, log=log, loss1_weight=1.0, loss2_weight=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "63F266sphX6R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUaoviPyhYJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fine tune 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B8D60yP5hYdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info('Fine tune Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34 net. Remove hard images. Use lovasz loss only with SGD. Less data aug')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLCZ7qsFhYYz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_config = '''\n",
        "p = Pipeline_Salt(min_mask_ratio=0.1)\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "#p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "#p.zoom(probability=0.5, min_factor=1.05, max_factor=1.2)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
        "\n",
        "#weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     #'sampler': weighted_sampler,\n",
        "                     'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n",
        "\n",
        "#saltnet = loaded_model\n",
        "#loss_fn_bce = nn.BCEWithLogitsLoss()\n",
        "loss_focal = FocalLoss(0.25, 2.0, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.SGD(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_lovasz_loss_se_finetune6_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_focal, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=300, print_every=44, save_model_every=20, save_log_every=2, log=log, loss1_weight=0.0, loss2_weight=1.)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nuk-O_sohYU4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info(model_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Le_5UumAnH_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U7EHw_lzhYQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt(min_mask_ratio=0.1)\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "#p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=3)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "#p.zoom(probability=0.5, min_factor=1.05, max_factor=1.2)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_random(probability=0.5, percentage_area=0.8, randomise_percentage_area=False)\n",
        "p.resize(probability=1, width=101, height=101, resample_filter='BILINEAR')\n",
        "\n",
        "#weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     #'sampler': weighted_sampler,\n",
        "                     'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFn5Yp8JiHrb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd ../salt_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EXgY-SnsiH6H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)\n",
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_se_finetune_2018_09_15_08_56_34.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "saltnet.load_state_dict(model_state_dict['model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKG-3gxECxlG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ej3k1zVvibz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#saltnet = loaded_model\n",
        "#loss_fn_bce = nn.BCEWithLogitsLoss()\n",
        "loss_focal = FocalLoss(0.25, 2.0, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.SGD(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_lovasz_loss_se_finetune6_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_focal, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=300, print_every=44, save_model_every=20, save_log_every=2, log=log, loss1_weight=0.0, loss2_weight=1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9MW3YwaZQmOz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Trained Model"
      ]
    },
    {
      "metadata": {
        "id": "ESBVbot1Qp7b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIRv-uXQGROz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loaded_model = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True, apply_se=True, r=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDpBS2c9ePeW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd ../salt_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuoeW6MLd439",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_se_2018_09_14_12_35_12.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "loaded_model.load_state_dict(model_state_dict['model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PbgxXtYH3KpR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Make Predictions on validation set"
      ]
    },
    {
      "metadata": {
        "id": "W0cFNMM0QmO4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set model to evaluation mode"
      ]
    },
    {
      "metadata": {
        "id": "izo8iByg3KpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loaded_model.eval()\n",
        "assert loaded_model.training == False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zfdN-P-j3KpS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_dataLoader = DataLoader(SaltDataset(X_val, y_val, depth_val, X_train_mean_img, out_size=128), batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7M0ZHG7o6swA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    loaded_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMuArCUd3KpU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_val_batch, y_val_batch, depth_val_batch, X_val_id_batch in val_dataLoader:\n",
        "        y_val_pred.append(loaded_model(X_val_batch))\n",
        "y_val_pred = torch.cat(y_val_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B7EYzvT0fyPx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt()\n",
        "p.flip_left_right(probability=1)\n",
        "val_dataLoader = DataLoader(SaltDataset(X_val, y_val, depth_val, X_train_mean_img, out_size=128, out_ch=1,\n",
        "                           transform=p.torch_transform()), batch_size=16)\n",
        "y_val_pred_flip = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_val_batch, y_val_batch, depth_val_batch, X_val_id_batch in val_dataLoader:\n",
        "        y_val_pred_flip.append(loaded_model(X_val_batch))\n",
        "y_val_pred_flip = torch.cat(y_val_pred_flip)\n",
        "y_val_pred_flip = torch.flip(y_val_pred_flip,[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FZNYVKKPf0if",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_ens = torch.where(y_val_pred.abs() > y_val_pred_flip.abs(), y_val_pred, y_val_pred_flip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hSYVoI88HApH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(0, X_val, y_val_pred.gt(0), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "230OrjoPf0_K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(0, X_val, y_val_pred_ens.gt(-0.1115), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VZvUQtjqG_uf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MASK_CUTOFF = -0.1115"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMy51k_WgCEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in np.r_[-0.15:-0.08:21j]:\n",
        "  print(i)\n",
        "  y_val_pred_adj = adjust_predictions(0, X_val, y_val_pred_ens.gt(i), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_ka-iqTgCYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxNTal6nS8Za",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataLoader = DataLoader(SaltDataset(X_train, y_train, depth_train, X_train_mean_img, out_size=128), batch_size=16)\n",
        "y_train_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_train_batch, y_train_batch, depth_train_batch, X_train_id_batch in train_dataLoader:\n",
        "        y_train_pred.append(loaded_model(X_train_batch))\n",
        "y_train_pred = torch.cat(y_train_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_DJpL1o3KpY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    rand_id = np.random.choice(X_val_id_batch)\n",
        "    print(f'Image ID: {rand_id}')\n",
        "    val_img = X_val[rand_id]/255\n",
        "    val_mask = y_val[rand_id]\n",
        "    val_mask_pred = y_val_pred.ge(0.5)[rand_id]\n",
        "    plot_img_mask_pred([val_img, val_mask, val_mask_pred], range(3), img_per_line=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkD6eqk9ghEe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    rand_id = np.random.choice(X_train_id_batch)\n",
        "    print(f'Image ID: {rand_id}')\n",
        "    img = X_train[rand_id]/255\n",
        "    mask = y_train[rand_id]\n",
        "    mask_pred = y_train_pred.ge(0.5)[rand_id]\n",
        "    plot_img_mask_pred([img, mask, mask_pred], range(3), img_per_line=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFW4BqZx8VzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ZERO_MASK_CUTOFF = 0\n",
        "MASK_CUTOFF = -0.1115"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMbnI4J6Jkr1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_val, y_val_pred.gt(MASK_CUTOFF), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aPOBqHPDVbzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(0, X_val, y_val_pred.gt(0.), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgcUBfKx8rpd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "for cut_off in range(0, 300, 10):\n",
        "  print(cut_off)\n",
        "  results.append(calc_mean_iou(adjust_predictions(cut_off, X_val, y_val_pred_ens.gt(MASK_CUTOFF), y_val.squeeze()), y_val.squeeze()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KcfUIfJyNna6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "range(0, 300, 10)[np.argmax(results)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1j58cIWfqxj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmT8DPBnGRPh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_val, y_val_pred.gt(MASK_CUTOFF), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCR_6IO1g7xE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_train, y_train_pred.gt(MASK_CUTOFF), y_train.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IeXdGnrPgIDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "for cut_off in range(0, 300, 10):\n",
        "  print(cut_off)\n",
        "  results.append(calc_mean_iou(adjust_predictions(cut_off, X_train, y_train_pred.gt(MASK_CUTOFF), y_train.squeeze()), y_train.squeeze()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQErtiLSgZPh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "range(0, 3000, 10)[np.argmax(results)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1oSze9o3Kp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make predictions on test set"
      ]
    },
    {
      "metadata": {
        "id": "c-M7XAbPhcUm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xzKpwGrhcOo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6ocZkIAS8Zf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test_dataLoader = DataLoader(SaltDataset(np_test[:10], None, depth_test, X_train_mean_img), batch_size=4)\n",
        "test_dataLoader = DataLoader(SaltDataset(X_test, np.zeros_like(X_test), depth_test, X_train_mean_img, out_size=128), batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChMRx1kh3KqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_raw = []\n",
        "with torch.no_grad():\n",
        "    for X_test_batch, y_test_batch, depth_test_batch, X_test_id_batch in test_dataLoader:\n",
        "        y_test_pred_raw.append(loaded_model(X_test_batch))\n",
        "y_test_pred = torch.cat(y_test_pred_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XiRFXS8K3KqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Show segmentation masks for a few images"
      ]
    },
    {
      "metadata": {
        "id": "Z8_mXXfA3KqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    rand_id = np.random.choice(X_test_id_batch)\n",
        "    print(f'Image ID: {rand_id}')\n",
        "    img = X_test[rand_id]/255\n",
        "    mask_pred = y_test_pred.ge(0.5)[rand_id]\n",
        "    plot_img_mask_pred([img, mask_pred], range(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OnjDYjvp3KqK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adjust predictions"
      ]
    },
    {
      "metadata": {
        "id": "btYQ-5_PlTBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " ZERO_MASK_CUTOFF = 0\n",
        " MASK_CUTOFF = -0.1115"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jojQbvhtS8Zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_test, y_test_pred.gt(MASK_CUTOFF))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M52jKbLA3KqM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode predictions using RLE(Run Length Encoding) method"
      ]
    },
    {
      "metadata": {
        "id": "uW0shno43KqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_rle = rle_encoder3d(y_test_pred_adj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdPQnJqUS8Zr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_adj.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9GaCKooz3KqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle = pd.DataFrame(index=misc_data['np_test_ids'], data=y_test_pred_rle).reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "loaXDzkM3KqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.columns = ['id', 'rle_mask']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_iM1oRr3KqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle[df_test_rle.rle_mask==''].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJzlb6q7cCM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09CFLpNL3KqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.to_csv(f'submission_{get_current_time_as_fname()}.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-yT6TajkR6U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_1B43JYGRQI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yV3mMqgRkYBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('submission_2018_09_15_00_18_33.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x38qXbAkmFSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KjPkHzKBmz7s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt()\n",
        "p.flip_left_right(probability=1)\n",
        "test_dataLoader = DataLoader(SaltDataset(X_test, np.zeros_like(X_test), depth_test, X_train_mean_img, out_size=128, out_ch=1,\n",
        "                           transform=p.torch_transform()), batch_size=16)\n",
        "y_test_pred_flip = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_test_batch, y_test_batch, depth_test_batch, X_test_id_batch in test_dataLoader:\n",
        "        y_test_pred_flip.append(loaded_model(X_test_batch))\n",
        "y_test_pred_flip = torch.cat(y_test_pred_flip)\n",
        "y_test_pred_flip = torch.flip(y_test_pred_flip,[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_zUtOPXwmz7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "y_test_pred = torch.where(y_test_pred.abs() > y_test_pred_flip.abs(), y_test_pred, y_test_pred_flip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSe7dmyMqnPF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_test, y_test_pred.gt(MASK_CUTOFF))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLO1ow7Hfejh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MASK_CUTOFF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fATv5NS6hnkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_rle = rle_encoder3d(y_test_pred_adj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BBNylCIDhnkX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_adj.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hwSpGmvthnkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle = pd.DataFrame(index=misc_data['np_test_ids'], data=y_test_pred_rle).reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3bS6FoNhnki",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.columns = ['id', 'rle_mask']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRDcCLpohnkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle[df_test_rle.rle_mask==''].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vkOmz86lhnkw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xr_NOtxBhnky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.to_csv(f'submission_{get_current_time_as_fname()}.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n__2CiMjhnk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCv-diichnk4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NgzFDdcShnlA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('submission_2018_09_15_17_45_21.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1D61fkPaiUR4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}