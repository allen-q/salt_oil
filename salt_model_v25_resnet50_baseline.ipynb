{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "salt_model_data_loader_V3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IUnFr6MO3Kk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Changes:\n",
        "1. Split train and val data using stratification based on mask pix density\n",
        "2. Over sample low pix density images due to zoom and crop\n",
        "3. Set positive class weight to 2"
      ]
    },
    {
      "metadata": {
        "id": "xaO5fG0VW5gB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install required packages if running on google colab"
      ]
    },
    {
      "metadata": {
        "id": "8n6EgF7sW5gC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    !pip install torch torchvision\n",
        "    !pip install imageio\n",
        "    !pip install Augmentor\n",
        "    !git clone https://github.com/allen-q/salt_oil.git\n",
        "    !git clone https://github.com/allen-q/salt_net.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p3h4PngQ0s86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd salt_oil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sq_wfOSpn3id",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVnBJygnW5gK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import required libs"
      ]
    },
    {
      "metadata": {
        "id": "x1VSamfH3Kk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from skimage import io, transform\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as ply\n",
        "import os\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import datetime as dt\n",
        "import pytz\n",
        "import pickle\n",
        "from salt_func_lib import *\n",
        "from torchvision import transforms, utils\n",
        "from skimage import io, transform\n",
        "import datetime as dt\n",
        "import sys\n",
        "from optparse import OptionParser\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from io import BytesIO\n",
        "import Augmentor\n",
        "import random\n",
        "import PIL\n",
        "import cv2 as cv\n",
        "% matplotlib inline\n",
        "% load_ext autoreload\n",
        "% autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I87qLhAOW5gO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Unet Modules"
      ]
    },
    {
      "metadata": {
        "id": "eC32auGDqVZi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pytorch_unet.eval import eval_net\n",
        "from pytorch_unet.unet import UNet\n",
        "from pytorch_unet.unet.unet_parts import *\n",
        "from pytorch_unet.unet.resnet import *\n",
        "from pytorch_unet.utils import get_ids, split_ids, split_train_val, get_imgs_and_masks, batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6O8Wz9H_iTDE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Setup data type based on whether GPU is enabled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QKYhIfCtEk6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    dtype = torch.cuda.FloatTensor ## UNCOMMENT THIS LINE IF YOU'RE ON A GPU!\n",
        "else:    \n",
        "    dtype = torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51wk3a_bTtv-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(f'Data Type set to: {dtype}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30EV6nbKbtyV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Global Variables"
      ]
    },
    {
      "metadata": {
        "id": "8zcHyi6AMfDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_global_variables():\n",
        "    \"\"\"initialize global variables such as db connection, logger etc.\"\"\"\n",
        "    global log\n",
        "    log = get_logger('SaltNet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLILkc8Vbtya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init_global_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DNoyvMPbFOF6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def adjust_brightness(img, alpha=None, beta=None):\n",
        "    if alpha is None:\n",
        "        # get a random num from 0.75 to 1.25\n",
        "        alpha = (random.random()/2)+0.75\n",
        "    if beta is None:\n",
        "        # get a random num from -40 to 40\n",
        "        beta = round((random.random()-0.5)*80)\n",
        "    #print(f'a:{alpha}, b:{beta}')\n",
        "    img_new = cv.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "    return img_new.reshape(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwNexNnZxQ7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SaltDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, np_img, np_mask, df_depth, mean_img, out_size=101, \n",
        "                 out_ch=1, transform=None, random_brightness=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (string): Path to the image files.\n",
        "            train (bool): Load train or test data\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.np_img = np_img\n",
        "        self.np_mask = np_mask.clip(0,1)\n",
        "        self.df_depth = df_depth\n",
        "        self.mean_img = mean_img\n",
        "        self.out_size = out_size\n",
        "        self.out_ch = out_ch\n",
        "        self.transform = transform\n",
        "        self.random_brightness = random_brightness\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.np_img)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, torch.Tensor):\n",
        "            idx = idx.item()\n",
        "            \n",
        "        X = self.np_img[idx]\n",
        "        #X = X - self.mean_img\n",
        "\n",
        "        if self.np_mask is None:\n",
        "            y = np.zeros((101,101,1))\n",
        "        else:\n",
        "            y = self.np_mask[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img_in = PIL.Image.fromarray(np.c_[np.tile(X, 2), y*255])\n",
        "            #img_in = PIL.Image.fromarray(np.tile(y, 3)*255)\n",
        "            transformed = np.array(self.transform(img_in))\n",
        "            #X = np.clip(transformed[:,:,0:1]/255, 0., 1.) - self.mean_img\n",
        "            X = transformed[:,:,0:1]\n",
        "            y = np.clip(transformed[:,:,2:3]/255, 0., 1.)\n",
        "            \n",
        "        if self.random_brightness > random.random():            \n",
        "            X = adjust_brightness(X)\n",
        "            X = np.clip(X/255, 0., 1.) - self.mean_img\n",
        "        else:\n",
        "            X = np.clip(X/255, 0., 1.) - self.mean_img            \n",
        "        #from boxx import g\n",
        "        #g()\n",
        "        X = np.moveaxis(X, -1,0)\n",
        "\n",
        "        pad_size = self.out_size - X.shape[2]\n",
        "        pad_first = pad_size//2\n",
        "        pad_last = pad_size - pad_first\n",
        "        X = np.pad(X, [(0, 0),(pad_first, pad_last), (pad_first, pad_last)], mode='reflect')\n",
        "\n",
        "        d = self.df_depth.iloc[idx,0]\n",
        "\n",
        "        X = torch.from_numpy(X).float().type(dtype)\n",
        "        X = X.repeat(self.out_ch,1,1)\n",
        "        y = transform.resize(y, (101, 101), mode='constant', preserve_range=True)\n",
        "        y = torch.from_numpy(y).float().squeeze().type(dtype)\n",
        "\n",
        "        return (X,y,d,idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q0_gBiSvxQ7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Pipeline_Salt(Augmentor.Pipeline):\n",
        "    def __init__(self, source_directory=None, output_directory=\"output\", save_format=None):\n",
        "        super(Pipeline_Salt, self).__init__(source_directory, output_directory, save_format)\n",
        "\n",
        "    def torch_transform(self):\n",
        "        \"\"\"\n",
        "        Returns the pipeline as a function that can be used with torchvision.\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            >>> import Augmentor\n",
        "            >>> import torchvision\n",
        "            >>> p = Augmentor.Pipeline()\n",
        "            >>> p.rotate(probability=0.7, max_left_rotate=10, max_right_rotate=10)\n",
        "            >>> p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "            >>> transforms = torchvision.transforms.Compose([\n",
        "            >>>     p.torch_transform(),\n",
        "            >>>     torchvision.transforms.ToTensor(),\n",
        "            >>> ])\n",
        "\n",
        "        :return: The pipeline as a function.\n",
        "        \"\"\"\n",
        "        def _transform(image):\n",
        "            for operation in self.operations:\n",
        "                r = round(random.uniform(0, 1), 1)\n",
        "                if r <= operation.probability:\n",
        "                    if not isinstance(image, list):\n",
        "                        image = [image]\n",
        "                    image = operation.perform_operation(image)[0]\n",
        "\n",
        "            return image\n",
        "\n",
        "        return _transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEXPdEFd3KmA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "metadata": {
        "id": "53yVOPsQ3KmB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load train and test data from npy files or from raw images if npy files not exist."
      ]
    },
    {
      "metadata": {
        "id": "wO1kf6HW3KmC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np_train_all, np_train_all_mask, X_test, misc_data = load_all_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fUAORGGdEgJx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate number of mask pixels per image"
      ]
    },
    {
      "metadata": {
        "id": "N6_End1QEgJy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train_mask = pd.DataFrame((np_train_all_mask/255).sum((1,2,3)), columns=['mask_pix'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ful85B7REgJ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train_mask.mask_pix = df_train_mask.mask_pix.round(-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Ao26rsVEgJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mask_pix_bins = df_train_mask.mask_pix.sort_values().unique()\n",
        "# Due to zooming and crop, under-sample all black and all white masks, over-sample images with small mask areas.\n",
        "mask_pix_bin_weights = ([1.] + np.r_[2:1:102j].tolist() + [0.2])\n",
        "mask_pix_bin_weights = dict(zip(mask_pix_bins, mask_pix_bin_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hyt5JXyrEgKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_all_sample_weight = df_train_mask.mask_pix.map(mask_pix_bin_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq_sH6YeEgKG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#np.log10(df_train_mask.mask_pix.div(100).add(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTkpzBqmGRN2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove black images"
      ]
    },
    {
      "metadata": {
        "id": "8BXkNsiZGRN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#black_img_ids = (np_train_all.max((1,2,3))==0)\n",
        "\n",
        "#np_train_all = np_train_all[~black_img_ids]\n",
        "#np_train_all_mask = np_train_all_mask[~black_img_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "obPeKNjDGRN6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np_train_all.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZqPs7VnYd56",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove images with all black masks"
      ]
    },
    {
      "metadata": {
        "id": "lzqSzGzEYd5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#black_mask_ids = (np_train_all_mask.max((1,2,3))==0)\n",
        "#np_train_all = np_train_all[~black_mask_ids]\n",
        "#np_train_all_mask = np_train_all_mask[~black_mask_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DNIS7zT23KmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Val data split"
      ]
    },
    {
      "metadata": {
        "id": "Q8HLh-bNQmNz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#np_train_all = np.clip(np_train_all/255, 0, 1)\n",
        "#X_test = np.clip(X_test/255, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqYjA-Ud3KmI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_ids, X_val_ids = (\n",
        "    train_test_split(df_train_mask.index.tolist(), \n",
        "                     test_size=0.20,\n",
        "                     stratify = df_train_mask.mask_pix,\n",
        "                     random_state=0)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GAWRi1bLpyM_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''with open('./data/df_train_img_iou.pickle', 'rb') as f:\n",
        "    df_train_img_iou = pickle.load(f)\n",
        "\n",
        "train_hard_img_id= (\n",
        "    [misc_data['np_train_all_ids'].index(e) for e in df_train_img_iou.loc[df_train_img_iou.type=='HARD'].id]\n",
        ")\n",
        "\n",
        "X_train_ids = np.setdiff1d(X_train_ids, train_hard_img_id)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GEqXO7GM3KmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = np_train_all[X_train_ids]\n",
        "X_val = np_train_all[X_val_ids]\n",
        "y_train = np_train_all_mask[X_train_ids]\n",
        "y_val = np_train_all_mask[X_val_ids]\n",
        "depth_train = (\n",
        "    misc_data['df_train_all_depth']\n",
        "    .reindex(np.array(misc_data['np_train_all_ids'])[X_train_ids])\n",
        ")\n",
        "depth_val = (\n",
        "    misc_data['df_train_all_depth']\n",
        "    .reindex(np.array(misc_data['np_train_all_ids'])[X_val_ids])\n",
        ")\n",
        "depth_test = (\n",
        "    misc_data['df_train_all_depth']\n",
        "    .reindex(np.array(misc_data['np_test_ids']))\n",
        ")\n",
        "#X_train_mean_img = X_train.mean(0).astype(np.float32)\n",
        "#X_train_mean_img = X_train.mean((0,1,2)).astype(np.float32)\n",
        "X_train_mean_img = np.clip(np_train_all/255, 0, 1).mean((0,1,2)).astype(np.float32)\n",
        "\n",
        "all_data = {\n",
        "    'X_train': X_train,\n",
        "    'X_val': X_val,\n",
        "    'y_train': y_train,\n",
        "    'y_val': y_val,\n",
        "    'X_test': X_test,\n",
        "    'X_train_mean_img': X_train_mean_img\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6BSSFObEgKl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_sample_weight = train_all_sample_weight[X_train_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ZVs5X-EpyNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_mean_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2L51fSDkpyNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zl5Q4dsOpyNa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "koLKXAYPpyNd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "depth_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0bMOyfebQmON",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt()\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=5)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "p.zoom(probability=0.5, min_factor=1.0, max_factor=1.5)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_by_size(probability=0.5, width=101, height=101, centre=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8eXIcYoDqVcF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a Train Dataloader"
      ]
    },
    {
      "metadata": {
        "id": "uYerp5hjW5gu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 16,\n",
        "                     'sampler': weighted_sampler,\n",
        "                     #'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 16,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n",
        "\n",
        "sample = iter(dataloaders['train']).__next__()\n",
        "\n",
        "assert sample[0].shape == torch.Size([train_data_params['batch_size'], 1, 128, 128])\n",
        "assert sample[1].shape == torch.Size([train_data_params['batch_size'], 101, 101])\n",
        "assert sample[2].shape == torch.Size([train_data_params['batch_size']])\n",
        "assert sample[3].shape == torch.Size([train_data_params['batch_size']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHZ6rNjxFOIB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = iter(train_dataLoader).__next__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fgi872V02Fsz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch, d_batch, X_id = t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dp1YZ7D5QmOg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for X_batch, y_batch, d_batch, X_id in dataloaders['train']:\n",
        "    i+=1\n",
        "    if i>30:\n",
        "        break\n",
        "    X_orig = X_train[X_id[0]].squeeze()/255\n",
        "    X_tsfm = X_batch[0,0].squeeze().cpu().detach().numpy()[13:114,13:114] + X_train_mean_img.squeeze()\n",
        "    y_orig = y_train[X_id[0]].squeeze()\n",
        "    y_tsfm = (y_batch[0].squeeze().cpu().detach().numpy())\n",
        "    print(f'{X_orig.sum()}, {X_tsfm.sum()}')\n",
        "    plot_img_mask_pred([X_orig, X_tsfm, y_orig, y_tsfm],\n",
        "                       [f'X Original-{X_id[0]}', 'X Transformed', 'y Original', 'y Transformed'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbN1-S12btDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JG0T8T1aGnGv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion1, criterion2, optimizer, scheduler, model_save_name, other_data={}, \n",
        "                num_epochs=25, print_every=2, save_model_every=None, save_log_every=None, log=get_logger('SaltNet'), loss2_weight=0):\n",
        "    #args = locals()\n",
        "    #args = {k:v.shape if isinstance(v, (torch.Tensor, np.ndarray)) else v for k,v in args.items()}\n",
        "    #args = {k:v.shape if isinstance(v, (torch.Tensor, np.ndarray)) else v for k,v in args.items()}\n",
        "    log.info('Start Training...')\n",
        "    #log.info('Passed parameters: {}'.format(args))\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_model = None\n",
        "    best_iou = 0.0\n",
        "    all_losses = []\n",
        "    iter_count = 0\n",
        "    X_train = other_data['X_train']\n",
        "    X_val = other_data['X_val']\n",
        "    y_train = other_data['y_train']\n",
        "    y_val = other_data['y_val']\n",
        "    X_train_mean_img = other_data['X_train_mean_img']\n",
        "    mask_cutoff = 0.\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        log.info('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        log.info('-' * 20)\n",
        "        if save_log_every is not None:\n",
        "            if (epoch % save_log_every == 0):\n",
        "                push_log_to_git()\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            epoch_loss1 = []\n",
        "            epoch_loss2 = []\n",
        "            pred_vs_true_epoch = []\n",
        "\n",
        "            for X_batch, y_batch, d_batch, X_id in dataloaders[phase]:\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    #y_pred, y_mask_pred = model(X_batch)\n",
        "                    y_pred = model(X_batch)\n",
        "                    pred_vs_true_epoch.append([y_pred, y_batch])\n",
        "                    \n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':                        \n",
        "                        #from boxx import g\n",
        "                        #g()\n",
        "                        #y_batch_mask_pix = y_batch.sum((1,2))\n",
        "                        #sample_weight = torch.where((y_batch_mask_pix<202) | (y_batch_mask_pix>5000), torch.tensor(1.0).type(dtype), (101**2/y_batch_mask_pix))\n",
        "                        #sample_weight = torch.ones_like(y_batch) * sample_weight.reshape(-1,1,1)\n",
        "                        \n",
        "                        loss_1 = criterion1(y_pred, y_batch.float())      \n",
        "                        #loss_1 = F.binary_cross_entropy_with_logits(y_pred, y_batch.float(), weight=sample_weight)\n",
        "                        if (epoch % 150 == 0):\n",
        "                          loss2_weight = loss2_weight + 0.3\n",
        "                        loss_2 = loss2_weight * criterion2(y_pred, y_batch.float()) \n",
        "\n",
        "                        #print(f'loss_pix: {loss_pix}, loss_dice:{loss_dice}')\n",
        "                        loss = loss_1 + loss_2\n",
        "                        #loss = loss_pix\n",
        "                        all_losses.append(loss.item())\n",
        "                        epoch_loss1.append(loss_1.item())\n",
        "                        epoch_loss2.append(loss_2.item())\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        iter_count += 1\n",
        "                if (phase == 'train') & (iter_count % print_every == 0):\n",
        "                    iou_batch = calc_mean_iou(y_pred.ge(mask_cutoff), y_batch.float())\n",
        "                    iou_acc = calc_clf_accuracy(y_pred.ge(mask_cutoff), y_batch.float())\n",
        "\n",
        "                    log.info('Batch Loss: {:.4f}, Epoch loss_1: {:.4f}, Epoch loss_2: {:.4f}, Batch IOU: {:.4f}, Batch Acc: {:.4f} at iter {}, epoch {}, Time: {}'.format(\n",
        "                        np.mean(all_losses[-print_every:]), np.mean(epoch_loss1), np.mean(epoch_loss2), iou_batch, iou_acc, iter_count, epoch, timeSince(start))\n",
        "                    )\n",
        "                    #print(all_losses)\n",
        "                    X_orig = X_train[X_id[0]].squeeze()/255\n",
        "                    X_tsfm = X_batch[0,0].squeeze().cpu().detach().numpy()\n",
        "                    X_tsfm = transform.resize(X_tsfm, (128, 128), mode='constant', preserve_range=True)\n",
        "                    X_tsfm = X_tsfm[13:114,13:114] + X_train_mean_img.squeeze()\n",
        "                    #X_tsfm = X_batch[0][X_batch[0].sum((1,2)).argmax()].squeeze().cpu().detach().numpy()[:101,:101] + X_train_mean_img.squeeze()\n",
        "\n",
        "                    y_orig = y_train[X_id[0]].squeeze()\n",
        "                    y_tsfm = (y_batch[0].squeeze().cpu().detach().numpy())\n",
        "                    y_tsfm_pred =  y_pred[0].squeeze().gt(mask_cutoff)\n",
        "                    plot_img_mask_pred([X_orig, X_tsfm, y_orig, y_tsfm, y_tsfm_pred],\n",
        "                                       ['X Original', 'X Transformed', 'y Original', 'y Transformed', 'y Predicted'])\n",
        "\n",
        "            y_pred_epoch = torch.cat([e[0] for e in pred_vs_true_epoch])\n",
        "            y_true_epoch = torch.cat([e[1] for e in pred_vs_true_epoch])\n",
        "            #from boxx import g\n",
        "            #g()\n",
        "            mean_iou_epoch = calc_mean_iou(y_pred_epoch.ge(mask_cutoff), y_true_epoch.float())\n",
        "            mean_acc_epoch = calc_clf_accuracy(y_pred_epoch.ge(mask_cutoff), y_true_epoch.float())\n",
        "            log.info('{} Mean IOU: {:.4f}, Mean Acc: {:.4f}, Best Val IOU: {:.4f} at epoch {}'.format(phase, mean_iou_epoch, mean_acc_epoch, best_iou, epoch))\n",
        "            if phase == 'val' and mean_iou_epoch > best_iou:\n",
        "                best_iou = mean_iou_epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                stats = {'best_iou': best_iou,\n",
        "                         'all_losses': all_losses,\n",
        "                         'iter_count': iter_count}\n",
        "                log.info(save_model_state_to_chunks(epoch, copy.deepcopy(model.state_dict()),\n",
        "                                                    copy.deepcopy(optimizer.state_dict()),\n",
        "                                                    copy.deepcopy(scheduler.state_dict()), stats, model_save_name, '.'))\n",
        "                best_model = (epoch, copy.deepcopy(model.state_dict()),\n",
        "                                                    copy.deepcopy(optimizer.state_dict()),\n",
        "                                                    copy.deepcopy(scheduler.state_dict()), stats, model_save_name, '.')\n",
        "                log.info('Best Val Mean IOU so far: {}'.format(best_iou))\n",
        "                # Visualize 1 val sample and predictions\n",
        "                X_orig = X_val[X_id[0]].squeeze()/255\n",
        "                y_orig = y_val[X_id[0]].squeeze()\n",
        "                y_pred2 =  y_pred[0].squeeze().gt(mask_cutoff)\n",
        "                plot_img_mask_pred([X_orig, y_orig, y_pred2],\n",
        "                                   ['Val X Original', 'Val y Original', 'Val y Predicted'])\n",
        "        if save_model_every is not None:\n",
        "            if (epoch % save_model_every == 0) | (epoch == num_epochs-1):\n",
        "                if (best_model is not None) and (best_iou>=0.8):\n",
        "                    log.info(save_model_state_to_chunks(*best_model))                \n",
        "                    push_model_to_git(ckp_name=model_save_name)\n",
        "                    best_model = None\n",
        "                else:\n",
        "                    log.info(\"Skip pushing model to git as there's no improvement\")\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    log.info('-' * 20)\n",
        "    time_elapsed = time.time() - start\n",
        "    log.info('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    log.info('Best val IOU: {:4f}'.format(best_iou))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qS4kZWQnW5gw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a Train Dataloader for sanity check"
      ]
    },
    {
      "metadata": {
        "id": "lP-yemq8pyOf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(depth_train['weight'][:8], 2)\n",
        "#weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values[:8], 2)\n",
        "train_data_params = {'batch_size': 8}\n",
        "val_data_params = {'batch_size': 8}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEA7t3xaQmOo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train[:8], y_train[:32], depth_train[:32],\n",
        "                           X_train_mean_img, out_size=128, out_ch=1,\n",
        "                           transform=None), **train_data_params)\n",
        "                           #transform=p.torch_transform()), **data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val[:8], y_val[:8], depth_val[:8], \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2hyd9ryGROf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = iter(train_dataLoader).__next__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v34uY0GkGROl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch, d_batch, X_id = t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dasEpZc0QmOt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model using a small data set to see if it can overfit"
      ]
    },
    {
      "metadata": {
        "id": "7SbmYNEfkWQB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PIaW0ComzkI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saltnet = resnet50unet()\n",
        "\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "#loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
        "\n",
        "model_save_name = None\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=100, print_every=1, save_model_every=None, save_log_every=None, log=log, loss2_weight=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zJ3DJ4hAQmOw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the full with full dataset"
      ]
    },
    {
      "metadata": {
        "id": "AZkSxKV2sIkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info('Build a UNET using resnet34 as backbone')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z3treVXh_Uha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_config = '''\n",
        "p = Pipeline_Salt()\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=5)\n",
        "p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\n",
        "p.zoom(probability=0.5, min_factor=1.0, max_factor=1.5)\n",
        "p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_by_size(probability=0.5, width=101, height=101, centre=False)\n",
        "\n",
        "\n",
        "saltnet = resnet34unet()\n",
        "\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "#loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.01)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_resnet_baseline_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=300, print_every=50, save_model_every=20, save_log_every=2, log=log, loss2_weight=0.1)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a563SYmgwbX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info(model_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dPFY13DHvAC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd ../salt_oil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u8EUxk8UQmOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saltnet = resnet34unet()\n",
        "\n",
        "loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "#loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.01)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_resnet_baseline_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_fn_bce, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=300, print_every=50, save_model_every=20, save_log_every=2, log=log, loss2_weight=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H2z9Brb8EgMG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-P-jcOQMjL7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "apTS0nKdR5WI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fine tune Scenario 1"
      ]
    },
    {
      "metadata": {
        "id": "6gH3StpeyZoc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info('Use less data aug, use focal and lovasz loss for fine-tuning.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1G7CTpsySfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_config = '''\n",
        "p = Pipeline_Salt()\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=2)\n",
        "p.rotate(probability=0.5, max_left_rotation=5, max_right_rotation=5)\n",
        "p.zoom(probability=0.5, min_factor=1.0, max_factor=1.1)\n",
        "p.shear(probability=0.5, max_shear_left=5, max_shear_right=5)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_by_size(probability=0.5, width=101, height=101, centre=False)\n",
        "\n",
        "weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     'sampler': weighted_sampler,\n",
        "                     #'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n",
        "\n",
        "saltnet = loaded_model\n",
        "#saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True)\n",
        "#loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_bce_loss_lovasz_loss_new_split_finetune1_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_focal, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=50, print_every=50, save_model_every=20, save_log_every=2, log=log, loss2_weight=1)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLGTRHvqzIMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info(model_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WZj2ujOdjrr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt()\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=2)\n",
        "p.rotate(probability=0.5, max_left_rotation=5, max_right_rotation=5)\n",
        "p.zoom(probability=0.5, min_factor=1.0, max_factor=1.1)\n",
        "p.shear(probability=0.5, max_shear_left=5, max_shear_right=5)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_by_size(probability=0.5, width=101, height=101, centre=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LhsJaCkmFQzv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9srB0xzQxwY2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     'sampler': weighted_sampler,\n",
        "                     #'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MfnNZyIMjr2z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1TB1NlPx-_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saltnet = loaded_model\n",
        "#saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True)\n",
        "#loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_bce_loss_lovasz_loss_new_split_finetune1_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_focal, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=50, print_every=50, save_model_every=20, save_log_every=2, log=log, loss2_weight=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HSjNRI8_Rv2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDh_3C6NSOKr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fine tune Scenario 2"
      ]
    },
    {
      "metadata": {
        "id": "HcszTYxDSOKt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info('Use more data aug, use focal and lovasz loss for fine-tuning.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lfc4OrrHSOKy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_config = '''\n",
        "p = Pipeline_Salt()\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=6)\n",
        "p.rotate(probability=0.5, max_left_rotation=20, max_right_rotation=20)\n",
        "p.zoom(probability=0.5, min_factor=1.0, max_factor=2.0)\n",
        "p.shear(probability=0.5, max_shear_left=20, max_shear_right=20)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_by_size(probability=0.5, width=101, height=101, centre=False)\n",
        "\n",
        "weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     'sampler': weighted_sampler,\n",
        "                     #'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}\n",
        "\n",
        "saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True)\n",
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_new_split_2018_09_12_09_40_55.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "saltnet.load_state_dict(model_state_dict['model'])\n",
        "if torch.cuda.is_available():\n",
        "    saltnet.cuda()\n",
        "    \n",
        "\n",
        "#saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True)\n",
        "#loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_bce_loss_lovasz_loss_new_split_finetune2_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_focal, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=50, print_every=50, save_model_every=20, save_log_every=2, log=log, loss2_weight=1)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nl8qOARZSOK0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log.info(model_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HM28H0KdSOK5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt()\n",
        "#p.skew(probability=1, magnitude=0.5)\n",
        "p.random_distortion(probability=0.5, grid_width=3, grid_height=3, magnitude=6)\n",
        "p.rotate(probability=0.5, max_left_rotation=20, max_right_rotation=20)\n",
        "p.zoom(probability=0.5, min_factor=1.0, max_factor=2.0)\n",
        "p.shear(probability=0.5, max_shear_left=20, max_shear_right=20)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.crop_by_size(probability=0.5, width=101, height=101, centre=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ef6xVB5cSOK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_SSchcR_SOK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weight.values, depth_train.shape[0])\n",
        "\n",
        "train_data_params = {'batch_size': 32,\n",
        "                     'sampler': weighted_sampler,\n",
        "                     #'shuffle': True,\n",
        "                     'drop_last': True}\n",
        "\n",
        "val_data_params = {'batch_size': 32,\n",
        "                   'shuffle': True,\n",
        "                   'drop_last': False}\n",
        "\n",
        "train_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_train, y_train, depth_train,\n",
        "                           X_train_mean_img, out_size=128,  out_ch=1,\n",
        "                           transform=p.torch_transform(), random_brightness=0.5), **train_data_params)\n",
        ")\n",
        "\n",
        "val_dataLoader = (\n",
        "    DataLoader(SaltDataset(X_val, y_val, depth_val, \n",
        "                           X_train_mean_img, out_size=128, out_ch=1), **val_data_params)\n",
        ")\n",
        "\n",
        "dataloaders = {'train': train_dataLoader, 'val':val_dataLoader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_AnGjHz5SOK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd ../salt_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UeNnACiFSOLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True)\n",
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_new_split_2018_09_12_09_40_55.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "saltnet.load_state_dict(model_state_dict['model'])\n",
        "if torch.cuda.is_available():\n",
        "    saltnet.cuda()\n",
        "    \n",
        "\n",
        "#saltnet = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True)\n",
        "#loss_fn_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.0).type(dtype))\n",
        "loss_focal = FocalLoss(0.25, 2, logits=True)\n",
        "loss_lovasz_hinge = LovaszHingeLoss()\n",
        "optimizer = torch.optim.Adam(saltnet.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "model_save_name = f'../salt_net/Unet_bce_loss_lovasz_loss_new_split_finetune2_{get_current_time_as_fname()}.ckp'\n",
        "log.info(model_save_name)\n",
        "\n",
        "# Test Run\n",
        "trained_model = train_model(saltnet, dataloaders, loss_focal, loss_lovasz_hinge, optimizer, scheduler, model_save_name, \n",
        "                other_data=all_data, num_epochs=50, print_every=50, save_model_every=20, save_log_every=2, log=log, loss2_weight=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FoO_ieSARwJ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6MWAYC86Rwb6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GeYClpSPR-La",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9MW3YwaZQmOz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Trained Model"
      ]
    },
    {
      "metadata": {
        "id": "LIRv-uXQGROz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loaded_model = UNet(n_channels=1, n_classes=1, bilinear=True, logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDpBS2c9ePeW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuoeW6MLd439",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_file_suffix = \"Unet_bce_loss_lovasz_loss_new_split_2018_09_12_09_40_55.ckp\"\n",
        "model_state_dict = torch.load(join_files(model_file_suffix, '.', returnFileObject=True, removeChunks=False))\n",
        "loaded_model.load_state_dict(model_state_dict['model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PbgxXtYH3KpR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Make Predictions on validation set"
      ]
    },
    {
      "metadata": {
        "id": "W0cFNMM0QmO4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set model to evaluation mode"
      ]
    },
    {
      "metadata": {
        "id": "izo8iByg3KpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loaded_model.eval()\n",
        "assert loaded_model.training == False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zfdN-P-j3KpS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_dataLoader = DataLoader(SaltDataset(X_val, y_val, depth_val, X_train_mean_img, out_size=128), batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7M0ZHG7o6swA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    loaded_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMuArCUd3KpU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_val_batch, y_val_batch, depth_val_batch, X_val_id_batch in val_dataLoader:\n",
        "        y_val_pred.append(loaded_model(X_val_batch))\n",
        "y_val_pred = torch.cat(y_val_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wVXNeOIGO5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt()\n",
        "p.flip_left_right(probability=1)\n",
        "val_dataLoader = DataLoader(SaltDataset(X_val, y_val, depth_val, X_train_mean_img, out_size=128, out_ch=1,\n",
        "                           transform=p.torch_transform()), batch_size=16)\n",
        "y_val_pred_flip = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_val_batch, y_val_batch, depth_val_batch, X_val_id_batch in val_dataLoader:\n",
        "        y_val_pred_flip.append(loaded_model(X_val_batch))\n",
        "y_val_pred_flip = torch.cat(y_val_pred_flip)\n",
        "y_val_pred_flip = torch.flip(y_val_pred_flip,[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "njStKyj9EuOy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5qJlPj4bLp6H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_ens = torch.where(y_val_pred.abs() > y_val_pred_flip.abs(), y_val_pred, y_val_pred_flip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5g3XHKULE1Hy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(0, X_val, y_val_pred_ens.gt(-0.4), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGQr61erKhxm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(0, X_val, y_val_pred.gt(-0.4), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wtSATxC2E06O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.where(y_val_pred.abs() > y_val_pred_flip.abs(), y_val_pred, y_val_pred_flip).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w-QlqhAVJoq8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred.abs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxNTal6nS8Za",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataLoader = DataLoader(SaltDataset(X_train, y_train, depth_train, X_train_mean_img, out_size=128), batch_size=16)\n",
        "y_train_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_train_batch, y_train_batch, depth_train_batch, X_train_id_batch in train_dataLoader:\n",
        "        y_train_pred.append(loaded_model(X_train_batch))\n",
        "y_train_pred = torch.cat(y_train_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_DJpL1o3KpY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    rand_id = np.random.choice(X_val_id_batch)\n",
        "    print(f'Image ID: {rand_id}')\n",
        "    val_img = X_val[rand_id]/255\n",
        "    val_mask = y_val[rand_id]\n",
        "    val_mask_pred = y_val_pred.ge(0.5)[rand_id]\n",
        "    plot_img_mask_pred([val_img, val_mask, val_mask_pred], range(3), img_per_line=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xaaXaJG5ErNd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4DmGOwRErix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkD6eqk9ghEe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    rand_id = np.random.choice(X_train_id_batch)\n",
        "    print(f'Image ID: {rand_id}')\n",
        "    img = X_train[rand_id]/255\n",
        "    mask = y_train[rand_id]\n",
        "    mask_pred = y_train_pred.ge(0.5)[rand_id]\n",
        "    plot_img_mask_pred([img, mask, mask_pred], range(3), img_per_line=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFW4BqZx8VzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ZERO_MASK_CUTOFF = 50\n",
        "MASK_CUTOFF = 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMbnI4J6Jkr1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_val, y_val_pred.gt(MASK_CUTOFF), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aPOBqHPDVbzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(0, X_val, y_val_pred.gt(0.), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgcUBfKx8rpd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "for cut_off in range(0, 300, 10):\n",
        "  print(cut_off)\n",
        "  results.append(calc_mean_iou(adjust_predictions(cut_off, X_val, y_val_pred_ens.gt(-0.4), y_val.squeeze()), y_val.squeeze()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KcfUIfJyNna6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "range(0, 300, 10)[np.argmax(results)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1j58cIWfqxj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmT8DPBnGRPh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_val, y_val_pred.gt(MASK_CUTOFF), y_val.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCR_6IO1g7xE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_train, y_train_pred.gt(MASK_CUTOFF), y_train.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IeXdGnrPgIDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "for cut_off in range(0, 300, 10):\n",
        "  print(cut_off)\n",
        "  results.append(calc_mean_iou(adjust_predictions(cut_off, X_train, y_train_pred.gt(MASK_CUTOFF), y_train.squeeze()), y_train.squeeze()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQErtiLSgZPh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "range(0, 3000, 10)[np.argmax(results)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1oSze9o3Kp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make predictions on test set"
      ]
    },
    {
      "metadata": {
        "id": "c-M7XAbPhcUm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xzKpwGrhcOo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6ocZkIAS8Zf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test_dataLoader = DataLoader(SaltDataset(np_test[:10], None, depth_test, X_train_mean_img), batch_size=4)\n",
        "test_dataLoader = DataLoader(SaltDataset(X_test, np.zeros_like(X_test), depth_test, X_train_mean_img, out_size=128), batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChMRx1kh3KqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_raw = []\n",
        "with torch.no_grad():\n",
        "    for X_test_batch, y_test_batch, depth_test_batch, X_test_id_batch in test_dataLoader:\n",
        "        y_test_pred_raw.append(loaded_model(X_test_batch))\n",
        "y_test_pred = torch.cat(y_test_pred_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0uZny9vMLSO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pipeline_Salt()\n",
        "p.flip_left_right(probability=1)\n",
        "test_dataLoader = DataLoader(SaltDataset(X_test, np.zeros_like(X_test), depth_test, X_train_mean_img, out_size=128, out_ch=1,\n",
        "                           transform=p.torch_transform()), batch_size=16)\n",
        "y_test_pred_flip = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_test_batch, y_test_batch, depth_test_batch, X_test_id_batch in test_dataLoader:\n",
        "        y_test_pred_flip.append(loaded_model(X_test_batch))\n",
        "y_test_pred_flip = torch.cat(y_test_pred_flip)\n",
        "y_test_pred_flip = torch.flip(y_test_pred_flip,[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ezFUHtIaNQ08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred = torch.where(y_test_pred.abs() > y_test_pred_flip.abs(), y_test_pred, y_test_pred_flip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XiRFXS8K3KqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Show segmentation masks for a few images"
      ]
    },
    {
      "metadata": {
        "id": "Z8_mXXfA3KqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    rand_id = np.random.choice(X_test_id_batch)\n",
        "    print(f'Image ID: {rand_id}')\n",
        "    img = X_test[rand_id]/255\n",
        "    mask_pred = y_test_pred.ge(0.5)[rand_id]\n",
        "    plot_img_mask_pred([img, mask_pred], range(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OnjDYjvp3KqK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adjust predictions"
      ]
    },
    {
      "metadata": {
        "id": "btYQ-5_PlTBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " ZERO_MASK_CUTOFF = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jojQbvhtS8Zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_adj = adjust_predictions(ZERO_MASK_CUTOFF, X_test, y_test_pred.gt(-0.4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M52jKbLA3KqM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode predictions using RLE(Run Length Encoding) method"
      ]
    },
    {
      "metadata": {
        "id": "uW0shno43KqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_rle = rle_encoder3d(y_test_pred_adj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdPQnJqUS8Zr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_adj.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9GaCKooz3KqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle = pd.DataFrame(index=misc_data['np_test_ids'], data=y_test_pred_rle).reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "loaXDzkM3KqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.columns = ['id', 'rle_mask']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_iM1oRr3KqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle[df_test_rle.rle_mask==''].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJzlb6q7cCM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09CFLpNL3KqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_rle.to_csv(f'submission_{get_current_time_as_fname()}.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-yT6TajkR6U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_1B43JYGRQI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yV3mMqgRkYBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('submission_2018_09_13_00_43_40.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x38qXbAkmFSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KjPkHzKBmz7s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_zUtOPXwmz7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSe7dmyMqnPF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}